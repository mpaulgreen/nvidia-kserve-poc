apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: meta-llama3-8b-instruct-24.05
spec:
  annotations:
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
  containers:
  - env:
    - name: NIM_CACHE_PATH # kserve volumes are readonly, fails. NIM continues to download the model for every pod.
      value: /mnt/models
    - name: NGC_API_KEY
      valueFrom:
        secretKeyRef:
           name: nvidia-nim-secrets
           key: NGC_API_KEY
    image: nvcr.io/nim/meta/llama3-8b-instruct:1.0.0
    name: kserve-container
    ports:
    - containerPort: 8000
      protocol: TCP
    resources:
      limits:
        cpu: "12"
        memory: 256Gi
      requests:
        cpu: "12"
        memory: 128Gi
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
  imagePullSecrets:
  - name: ngc-secret
  protocolVersions:
  - v2
  - grpc-v2
  supportedModelFormats:
  - autoSelect: true
    name: meta-llama3-8b-instruct
    priority: 1
    version: "24.05"
  volumes:
  - name: dshm
    emptyDir:
      medium: Memory
      sizeLimit: 128Gi
